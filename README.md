# ğŸ“„ Multi-Modal Document Intelligence (RAG-Based QA System)

## ğŸš€ Project Overview

Modern real-world documents such as financial reports, policy papers (e.g., IMF Article IV reports), and enterprise contracts contain **text, tables, charts, figures, scanned images, and footnotes**. Traditional text-only QA systems fail to capture insights from such heterogeneous data.

This project implements a **Multi-Modal Retrieval-Augmented Generation (RAG) system** that can accurately answer user questions by jointly reasoning over **text, tables, and images (via OCR)** with proper **source citations**.

---

## ğŸ¯ Key Objectives

* Build a **multi-modal document ingestion pipeline** (text, tables, images, OCR)
* Design a **smart chunking strategy** for complex document layouts
* Create a **unified embedding space** for multi-modal data
* Implement a **vector-based retrieval system**
* Develop a **QA chatbot** that produces **context-grounded, citation-backed answers**

---

## ğŸ§  System Architecture (High-Level)

```
PDF / Document
   â†“
Multi-Modal Ingestion
(text + tables + images + OCR)
   â†“
Smart Chunking
(semantic + layout-aware)
   â†“
Embeddings (Text & Vision)
   â†“
Vector Database (ChromaDB)
   â†“
Retriever + Re-ranking
   â†“
LLM (Groq / LLaMA / Mixtral)
   â†“
Answer + Page-Level Citations
   â†“
Streamlit / FastAPI UI
```

---

## ğŸ§© Core Components

### 1ï¸âƒ£ Document Ingestion

* **Text Extraction:** PyMuPDF (fitz)
* **Table Parsing:** Row/column-aware structured extraction
* **Image Handling:** Image extraction + preprocessing with Pillow
* **OCR:** Converts scanned pages and figures into searchable text
* **Metadata:** Page number, section, modality preserved

### 2ï¸âƒ£ Smart Chunking Strategy

* Semantic chunking (meaning-based)
* Layout-aware segmentation (page, section, table boundaries)
* Multi-modal alignment (text + table + OCR text)

Each chunk contains:

* Content
* Modality type (text / table / image)
* Source metadata (page, section)

### 3ï¸âƒ£ Embedding Strategy

* **Text & Tables:** Sentence-Transformers / HuggingFace embeddings
* **Images:** OCR text + image metadata embeddings
* **Unified Vector Space:** Enables cross-modal retrieval

### 4ï¸âƒ£ Vector Store & Retrieval

* **Vector DB:** ChromaDB
* **Retrieval:** Top-k semantic similarity search
* **Advanced (Bonus):**

  * Cross-modal reranking
  * Hybrid retrieval (RRF)

### 5ï¸âƒ£ RAG-Based Answer Generation

* **Framework:** LangChain
* **LLMs:** Groq-hosted LLaMA / Mixtral
* **Output:**

  * Faithful answers grounded in retrieved context
  * Page or section-level citations

### 6ï¸âƒ£ QA Interface

* **UI:** Streamlit / FastAPI
* Upload documents
* Ask natural language questions
* View answers with cited sources

---

## ğŸ› ï¸ Tech Stack

* **Frontend:** Streamlit
* **LLM Orchestration:** LangChain
* **LLMs:** Groq (LLaMA / Mixtral)
* **Embeddings:** Sentence-Transformers, HuggingFace
* **Vector Database:** ChromaDB
* **PDF Processing:** PyMuPDF
* **Image Processing:** Pillow
* **OCR:** Integrated OCR pipeline
* **Config Management:** python-dotenv

---

## ğŸ“¦ Installation

```bash
# Clone repository
git clone <repo-url>
cd multi-modal-rag

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ” Environment Variables

Create a `.env` file:

```env
GROQ_API_KEY=your_api_key_here
```

---

## â–¶ï¸ Run the Application

```bash
streamlit run app.py
```

Or (if using FastAPI):

```bash
uvicorn main:app --reload
```

---

## ğŸ“Š Evaluation & Benchmarking

* Benchmark queries across:

  * Text-based questions
  * Table-driven numerical queries
  * Image / OCR-based questions
* Metrics tracked:

  * Retrieval relevance
  * Answer faithfulness
  * Latency

---

## âœ¨ Bonus Features (Excellence Track)

* Cross-modal reranking using vision-text embeddings
* Hybrid retrieval (RRF)
* Summarization / briefing generation
* Retrieval & latency evaluation dashboard

---

## ğŸ“ Deliverables

* âœ… Modular, well-documented codebase
* âœ… Interactive QA demo
* âœ… Technical report (architecture & findings)
* âœ… Video demonstration (3â€“5 mins)

---



---

## ğŸ‘¤ Author

**Hardik Somani**
Data Science & GenAI Engineer
ğŸ“ Rajasthan, India

---

## ğŸ“œ License

This project is for educational and demonstration purposes.
